# Generated by Antigravity [Forge: Gemini 3.0] | Spec: [Architect: Opus 4.5]
"""Research Agent - LangGraph-based implementation for local Ollama deployment.

This module creates a deep research agent using LangGraph directly,
avoiding deepagents middleware compatibility issues with local models.

Uses local Ollama LLM with proper tool binding.
"""

import os
from datetime import datetime
from typing import Annotated, TypedDict, List, Dict, Any
from dotenv import load_dotenv

from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from research_agent.prompts import (
    RESEARCHER_INSTRUCTIONS,
    RESEARCH_WORKFLOW_INSTRUCTIONS,
    SUBAGENT_DELEGATION_INSTRUCTIONS,
)
from research_agent.tools import tavily_search, think_tool

# Load environment variables
load_dotenv()

# Configuration from environment
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen3:14b")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
MAX_CONCURRENT_RESEARCH_UNITS = int(os.getenv("MAX_CONCURRENT_RESEARCH_UNITS", "3"))
MAX_RESEARCHER_ITERATIONS = int(os.getenv("MAX_RESEARCHER_ITERATIONS", "3"))
RECURSION_LIMIT = int(os.getenv("RECURSION_LIMIT", "100"))

# Get current date for prompts
current_date = datetime.now().strftime("%Y-%m-%d")

# Define state
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    research_findings: List[str]
    iteration_count: int


# System prompt combining workflow and researcher instructions
SYSTEM_PROMPT = f"""You are a deep research assistant. Today's date is {current_date}.

Your task is to thoroughly research the user's question using web search.

WORKFLOW:
1. Think about what you need to search for
2. Use tavily_search to find relevant information  
3. Use think_tool to reflect on what you found and decide next steps
4. **You MUST perform at least 15-20 searches** before providing your final answer
5. Explore multiple angles: news, analysis, expert opinions, data sources, historical context
6. When you have comprehensive coverage from multiple sources, provide a detailed answer

RESEARCH DEPTH REQUIREMENTS:
- **Minimum 15-20 different searches** covering different aspects of the topic
- **Use think_tool after every 2-3 searches** to assess progress and plan next searches
- **Only conclude when you have 10+ unique, high-quality sources**
- **Your final answer MUST be 1500+ words** with detailed analysis and inline citations
- DO NOT provide a final answer until you have completed at least 15 searches

SEARCH STRATEGY:
- Search 1-3: Overview and recent news
- Search 4-6: Expert analysis and opinions
- Search 7-9: Historical context and trends
- Search 10-12: Data sources and statistics
- Search 13-15: Contrarian views and alternative perspectives
- Search 16-20: Deep dives into specific aspects discovered

After each think_tool reflection, explicitly count how many searches you've done. If under 15, continue searching.

Always cite your sources with URLs in [1], [2], [3] format.
"""


def create_agent():
    """Create a LangGraph-based research agent."""
    
    # Configure local Ollama model with tools
    model = ChatOllama(
        model=OLLAMA_MODEL,
        base_url=OLLAMA_BASE_URL,
        temperature=0.0,
    )
    
    tools = [tavily_search, think_tool]
    model_with_tools = model.bind_tools(tools)
    
    # Define the agent node
    def agent_node(state: AgentState) -> Dict[str, Any]:
        """Process the current state and decide next action."""
        messages = state["messages"]
        
        print(f"\n[AGENT] Processing state with {len(messages)} messages")
        print(f"[AGENT] Iteration: {state.get('iteration_count', 0)}")
        
        # Add system prompt as first message if not present
        if not messages or not any(
            hasattr(m, 'content') and SYSTEM_PROMPT[:50] in str(m.content) 
            for m in messages[:2]
        ):
            system_msg = HumanMessage(content=f"System: {SYSTEM_PROMPT}")
            messages = [system_msg] + list(messages)
        
        print(f"[AGENT] Invoking model: {OLLAMA_MODEL}")
        
        # Get model response with retry for empty responses
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = model_with_tools.invoke(messages)
                
                # Check if response has content or tool calls
                has_content = hasattr(response, 'content') and response.content
                has_tools = hasattr(response, 'tool_calls') and response.tool_calls
                
                print(f"[AGENT] Response attempt {attempt+1}: has_content={has_content}, has_tools={has_tools}")
                if has_content:
                    print(f"[AGENT] Content preview: {str(response.content)[:200]}...")
                if has_tools:
                    print(f"[AGENT] Tool calls: {response.tool_calls}")
                
                if has_content or has_tools:
                    break
                
                # If empty, add a prompt to encourage a response
                if attempt < max_retries - 1:
                    print(f"[AGENT] Empty response, retrying...")
                    messages = messages + [HumanMessage(content="You MUST use the tavily_search tool to search for information. Please search now.")]
            except Exception as e:
                print(f"[AGENT] Error during invoke: {e}")
                import traceback
                traceback.print_exc()
                if attempt == max_retries - 1:
                    raise
        
        # If still empty after retries, create a fallback response
        if not (has_content or has_tools):
            print("[AGENT] All retries exhausted, using fallback response")
            response = AIMessage(content="I apologize, but I'm having trouble processing this request. Please try rephrasing your question.")
        
        return {
            "messages": [response],
            "iteration_count": state.get("iteration_count", 0) + 1,
        }
    
    # Define routing logic
    def should_continue(state: AgentState) -> str:
        """Determine if we should continue or end."""
        messages = state["messages"]
        last_message = messages[-1] if messages else None
        
        print(f"\n[ROUTER] Checking routing decision...")
        print(f"[ROUTER] Iteration count: {state.get('iteration_count', 0)}")
        
        # Check iteration limit
        if state.get("iteration_count", 0) >= RECURSION_LIMIT:
            print(f"[ROUTER] Reached recursion limit, ending")
            return "end"
        
        # If last message has tool calls, continue to tools
        if last_message and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            print(f"[ROUTER] Has tool calls, routing to tools")
            return "tools"
        
        # Otherwise, we're done
        print(f"[ROUTER] No tool calls, ending")
        return "end"
    
    # Build the graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", ToolNode(tools))
    
    # Add edges
    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END,
        }
    )
    workflow.add_edge("tools", "agent")
    
    # Compile
    return workflow.compile()


# Create global agent instance
agent = create_agent()


def get_agent():
    """Get the configured research agent instance.
    
    Returns:
        The LangGraph research agent ready for invocation.
    """
    return agent


def get_agent_config():
    """Get the current agent configuration.
    
    Returns:
        Dictionary with current configuration settings.
    """
    return {
        "model": OLLAMA_MODEL,
        "base_url": OLLAMA_BASE_URL,
        "max_concurrent_research_units": MAX_CONCURRENT_RESEARCH_UNITS,
        "max_researcher_iterations": MAX_RESEARCHER_ITERATIONS,
        "recursion_limit": RECURSION_LIMIT,
    }
