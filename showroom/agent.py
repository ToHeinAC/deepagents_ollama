# Generated by Antigravity [Forge: Gemini 3.0] | Spec: [Architect: Opus 4.5]
"""Research Agent - LangGraph-based implementation for local Ollama deployment.

This module creates a deep research agent using LangGraph directly,
avoiding deepagents middleware compatibility issues with local models.

Uses local Ollama LLM with proper tool binding.
"""

import os
from datetime import datetime
from typing import Annotated, TypedDict, List, Dict, Any
from dotenv import load_dotenv

from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from research_agent.prompts import (
    RESEARCHER_INSTRUCTIONS,
    RESEARCH_WORKFLOW_INSTRUCTIONS,
    SUBAGENT_DELEGATION_INSTRUCTIONS,
)
from research_agent.tools import tavily_search, think_tool

# Load environment variables
load_dotenv()

# Configuration from environment
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "Qwen3:14b")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
MAX_CONCURRENT_RESEARCH_UNITS = int(os.getenv("MAX_CONCURRENT_RESEARCH_UNITS", "3"))
MAX_RESEARCHER_ITERATIONS = int(os.getenv("MAX_RESEARCHER_ITERATIONS", "3"))
RECURSION_LIMIT = int(os.getenv("RECURSION_LIMIT", "50"))

# Get current date for prompts
current_date = datetime.now().strftime("%Y-%m-%d")

# Define state
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    research_findings: List[str]
    iteration_count: int


# System prompt combining workflow and researcher instructions
SYSTEM_PROMPT = f"""You are a deep research assistant. Today's date is {current_date}.

Your task is to thoroughly research the user's question using web search.

WORKFLOW:
1. Think about what you need to search for
2. Use tavily_search to find relevant information  
3. Use think_tool to reflect on what you found and decide next steps
4. Repeat searches if needed (max 5 searches)
5. When you have enough information, provide a comprehensive answer

Always cite your sources with URLs.
"""


def create_agent():
    """Create a LangGraph-based research agent."""
    
    # Configure local Ollama model with tools
    model = ChatOllama(
        model=OLLAMA_MODEL,
        base_url=OLLAMA_BASE_URL,
        temperature=0.0,
    )
    
    tools = [tavily_search, think_tool]
    model_with_tools = model.bind_tools(tools)
    
    # Define the agent node
    def agent_node(state: AgentState) -> Dict[str, Any]:
        """Process the current state and decide next action."""
        messages = state["messages"]
        
        # Add system prompt as first message if not present
        if not messages or not any(
            hasattr(m, 'content') and SYSTEM_PROMPT[:50] in str(m.content) 
            for m in messages[:2]
        ):
            system_msg = HumanMessage(content=f"System: {SYSTEM_PROMPT}")
            messages = [system_msg] + list(messages)
        
        # Get model response with retry for empty responses
        max_retries = 3
        for attempt in range(max_retries):
            response = model_with_tools.invoke(messages)
            
            # Check if response has content or tool calls
            has_content = hasattr(response, 'content') and response.content
            has_tools = hasattr(response, 'tool_calls') and response.tool_calls
            
            if has_content or has_tools:
                break
            
            # If empty, add a prompt to encourage a response
            if attempt < max_retries - 1:
                messages = messages + [HumanMessage(content="Please provide a response or use a tool to search for information.")]
        
        # If still empty after retries, create a fallback response
        if not (has_content or has_tools):
            response = AIMessage(content="I apologize, but I'm having trouble processing this request. Please try rephrasing your question.")
        
        return {
            "messages": [response],
            "iteration_count": state.get("iteration_count", 0) + 1,
        }
    
    # Define routing logic
    def should_continue(state: AgentState) -> str:
        """Determine if we should continue or end."""
        messages = state["messages"]
        last_message = messages[-1] if messages else None
        
        # Check iteration limit
        if state.get("iteration_count", 0) >= RECURSION_LIMIT:
            return "end"
        
        # If last message has tool calls, continue to tools
        if last_message and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "tools"
        
        # Otherwise, we're done
        return "end"
    
    # Build the graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", ToolNode(tools))
    
    # Add edges
    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END,
        }
    )
    workflow.add_edge("tools", "agent")
    
    # Compile
    return workflow.compile()


# Create global agent instance
agent = create_agent()


def get_agent():
    """Get the configured research agent instance.
    
    Returns:
        The LangGraph research agent ready for invocation.
    """
    return agent


def get_agent_config():
    """Get the current agent configuration.
    
    Returns:
        Dictionary with current configuration settings.
    """
    return {
        "model": OLLAMA_MODEL,
        "base_url": OLLAMA_BASE_URL,
        "max_concurrent_research_units": MAX_CONCURRENT_RESEARCH_UNITS,
        "max_researcher_iterations": MAX_RESEARCHER_ITERATIONS,
        "recursion_limit": RECURSION_LIMIT,
    }
