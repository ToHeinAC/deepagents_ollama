# Generated by Antigravity [Forge: Gemini 3.0] | Spec: [Architect: Opus 4.5]
"""Research Agent - LangGraph-based implementation for local Ollama deployment.

This module creates a deep research agent using LangGraph directly,
avoiding deepagents middleware compatibility issues with local models.

Uses local Ollama LLM with proper tool binding.
"""

import os
from datetime import datetime
from typing import Annotated, TypedDict, List, Dict, Any
from dotenv import load_dotenv

from langchain_ollama import ChatOllama

# Import memory management utilities
try:
    from memory_utils import clear_cuda_memory, get_memory_stats
except ImportError:
    # Fallback if module not found
    import gc
    def clear_cuda_memory(verbose=True):
        gc.collect()
        if verbose:
            print("[MEMORY] Garbage collection completed (fallback)")
    def get_memory_stats():
        return None
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from research_agent.prompts import (
    RESEARCHER_INSTRUCTIONS,
    RESEARCH_WORKFLOW_INSTRUCTIONS,
    SUBAGENT_DELEGATION_INSTRUCTIONS,
)
from research_agent.tools import tavily_search, think_tool, submit_final_answer

# Load environment variables
load_dotenv()

# Configuration from environment
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen3:14b")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
MAX_CONCURRENT_RESEARCH_UNITS = int(os.getenv("MAX_CONCURRENT_RESEARCH_UNITS", "3"))
MAX_RESEARCHER_ITERATIONS = int(os.getenv("MAX_RESEARCHER_ITERATIONS", "3"))
RECURSION_LIMIT = int(os.getenv("RECURSION_LIMIT", "100"))

# Get current date for prompts
current_date = datetime.now().strftime("%Y-%m-%d")

# Define state
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    research_findings: List[str]
    iteration_count: int


# System prompt combining workflow and researcher instructions
SYSTEM_PROMPT = f"""You are a deep research assistant. Today's date is {current_date}.

Your task is to thoroughly research the user's question using web search and then submit a comprehensive final answer.

## AVAILABLE TOOLS
1. **tavily_search**: Search the web for information
2. **think_tool**: Reflect on findings and plan next steps (use after every 2-3 searches)
3. **submit_final_answer**: Submit your final comprehensive answer (REQUIRED to end the session)

## RESEARCH WORKFLOW
Follow this exact workflow:

### Phase 1: Planning (1 think_tool call)
- Create a research plan with 5-7 specific tasks
- Identify key aspects to investigate

### Phase 2: Research (15-20 searches minimum)
- Search 1-3: Overview and recent news
- Search 4-6: Expert analysis and opinions  
- Search 7-9: Historical context and trends
- Search 10-12: Data sources and statistics
- Search 13-15: Contrarian views and alternative perspectives
- Search 16-20: Deep dives into specific aspects discovered

### Phase 3: Reflection (think_tool after every 2-3 searches)
- Track your search count explicitly
- Assess what information you have vs. what's missing
- Plan remaining searches

### Phase 4: Final Answer (MANDATORY - use submit_final_answer tool)
**CRITICAL: You MUST use the submit_final_answer tool to end the research session.**
DO NOT just write your answer as a message - the session will NOT end properly.

## FINAL ANSWER REQUIREMENTS
Before calling submit_final_answer, ensure your answer meets ALL requirements:
- ✅ At least 1000 words (comprehensive, detailed analysis)
- ✅ At least 5 unique citations [1], [2], [3], [4], [5]
- ✅ All planned research tasks completed
- ✅ Multiple perspectives covered
- ✅ Well-structured with clear sections

If submit_final_answer rejects your submission, you MUST:
1. Continue researching to gather more information
2. Expand your answer to meet the word count
3. Add more citations from your sources
4. Try submitting again

## CITATION FORMAT
Use inline citations: [1], [2], [3]
End with a Sources section:
### Sources
[1] Title: URL
[2] Title: URL
...

## IMPORTANT RULES
- NEVER provide a final answer without using submit_final_answer tool
- NEVER stop researching until you have 15+ searches and 5+ sources
- ALWAYS use think_tool to track progress after every 2-3 searches
- The research session ONLY ends when submit_final_answer is accepted
"""


def create_agent():
    """Create a LangGraph-based research agent."""
    
    # Configure local Ollama model with tools
    model = ChatOllama(
        model=OLLAMA_MODEL,
        base_url=OLLAMA_BASE_URL,
        temperature=0.0,
    )
    
    tools = [tavily_search, think_tool, submit_final_answer]
    model_with_tools = model.bind_tools(tools)
    
    # Define the agent node
    def agent_node(state: AgentState) -> Dict[str, Any]:
        """Process the current state and decide next action."""
        messages = state["messages"]
        
        print(f"\n[AGENT] Processing state with {len(messages)} messages")
        print(f"[AGENT] Iteration: {state.get('iteration_count', 0)}")
        
        # Add system prompt as first message if not present
        if not messages or not any(
            hasattr(m, 'content') and SYSTEM_PROMPT[:50] in str(m.content) 
            for m in messages[:2]
        ):
            system_msg = HumanMessage(content=f"System: {SYSTEM_PROMPT}")
            messages = [system_msg] + list(messages)
        
        print(f"[AGENT] Invoking model: {OLLAMA_MODEL}")
        
        # Clear memory before model invocation to ensure resources available
        clear_cuda_memory(verbose=True)
        
        # Get model response with retry for empty responses
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = model_with_tools.invoke(messages)
                
                # Check if response has content or tool calls
                has_content = hasattr(response, 'content') and response.content
                has_tools = hasattr(response, 'tool_calls') and response.tool_calls
                
                print(f"[AGENT] Response attempt {attempt+1}: has_content={has_content}, has_tools={has_tools}")
                if has_content:
                    print(f"[AGENT] Content preview: {str(response.content)[:200]}...")
                if has_tools:
                    print(f"[AGENT] Tool calls: {response.tool_calls}")
                
                if has_content or has_tools:
                    break
                
                # If empty, add a prompt to encourage a response
                if attempt < max_retries - 1:
                    print(f"[AGENT] Empty response, retrying...")
                    messages = messages + [HumanMessage(content="You MUST use the tavily_search tool to search for information. Please search now.")]
            except Exception as e:
                print(f"[AGENT] Error during invoke: {e}")
                import traceback
                traceback.print_exc()
                if attempt == max_retries - 1:
                    raise
        
        # If still empty after retries, create a fallback response
        if not (has_content or has_tools):
            print("[AGENT] All retries exhausted, using fallback response")
            response = AIMessage(content="I apologize, but I'm having trouble processing this request. Please try rephrasing your question.")
        
        # Clear memory after model invocation
        clear_cuda_memory(verbose=True)
        
        return {
            "messages": [response],
            "iteration_count": state.get("iteration_count", 0) + 1,
        }
    
    # Define routing logic
    def should_continue(state: AgentState) -> str:
        """Determine if we should continue or end."""
        messages = state["messages"]
        last_message = messages[-1] if messages else None
        
        print(f"\n[ROUTER] Checking routing decision...")
        print(f"[ROUTER] Iteration count: {state.get('iteration_count', 0)}")
        
        # Check iteration limit
        if state.get("iteration_count", 0) >= RECURSION_LIMIT:
            print(f"[ROUTER] Reached recursion limit, ending")
            return "end"
        
        # Check if final answer was accepted (in tool results)
        for msg in reversed(messages[-5:]):
            if hasattr(msg, 'content') and isinstance(msg.content, str):
                if 'FINAL_ANSWER_ACCEPTED' in msg.content:
                    print(f"[ROUTER] Final answer accepted, ending research session")
                    return "end"
        
        # If last message has tool calls, continue to tools
        if last_message and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            # Check if it's a submit_final_answer call - route to tools for validation
            for tc in last_message.tool_calls:
                if tc.get('name') == 'submit_final_answer':
                    print(f"[ROUTER] Final answer submission detected, routing to tools for validation")
            print(f"[ROUTER] Has tool calls, routing to tools")
            return "tools"
        
        # FALLBACK: If agent outputs substantial markdown content, accept it as final answer
        # This handles models that don't properly use tool calls
        if last_message and hasattr(last_message, 'content') and last_message.content:
            content = str(last_message.content)
            # Check if it looks like a final answer (has headings and substantial length)
            if ('###' in content or '## ' in content) and len(content) > 500:
                print(f"[ROUTER] Agent output substantial markdown answer ({len(content)} chars), accepting as final")
                return "end"
        
        # If no tool calls but no accepted final answer, force agent to use submit_final_answer
        # But limit remind attempts to avoid infinite loops
        remind_count = sum(1 for msg in messages if hasattr(msg, 'content') and 'CRITICAL ERROR' in str(msg.content))
        if remind_count >= 3:
            print(f"[ROUTER] Already reminded {remind_count} times, ending to avoid loop")
            return "end"
        
        print(f"[ROUTER] No tool calls and no accepted final answer - agent should use submit_final_answer tool")
        return "continue_without_answer"
    
    # Node to remind agent to use submit_final_answer
    def remind_to_submit(state: AgentState) -> Dict[str, Any]:
        """Remind the agent that it must use submit_final_answer tool."""
        print(f"\n[REMIND] Agent tried to end without using submit_final_answer tool")
        reminder = HumanMessage(content="""CRITICAL ERROR: You wrote your answer as text instead of using the submit_final_answer tool.

You MUST make a TOOL CALL to submit_final_answer. Do NOT write JSON or text.

Call the tool like this:
- Tool: submit_final_answer
- Arguments:
  - answer: Your complete answer (300+ words, 5+ source URLs)
  - completed_tasks: Summary of what you researched

DO NOT output JSON text. Make an actual tool call to submit_final_answer NOW.""")
        return {
            "messages": [reminder],
            "iteration_count": state.get("iteration_count", 0) + 1,
        }
    
    # Build the graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", ToolNode(tools))
    workflow.add_node("remind", remind_to_submit)
    
    # Add edges
    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "continue_without_answer": "remind",
            "end": END,
        }
    )
    workflow.add_edge("tools", "agent")
    workflow.add_edge("remind", "agent")
    
    # Compile
    return workflow.compile()


# Create global agent instance
agent = create_agent()


def get_agent():
    """Get the configured research agent instance.
    
    Returns:
        The LangGraph research agent ready for invocation.
    """
    return agent


def get_agent_config():
    """Get the current agent configuration.
    
    Returns:
        Dictionary with current configuration settings.
    """
    return {
        "model": OLLAMA_MODEL,
        "base_url": OLLAMA_BASE_URL,
        "max_concurrent_research_units": MAX_CONCURRENT_RESEARCH_UNITS,
        "max_researcher_iterations": MAX_RESEARCHER_ITERATIONS,
        "recursion_limit": RECURSION_LIMIT,
    }
